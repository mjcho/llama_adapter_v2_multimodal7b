{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Testing \n",
    "Testing repos, models, and queries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaMA Adaptor V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import cv2\n",
    "import llama\n",
    "import torch\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 393M/393M [00:05<00:00, 75.0MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLaMA-Adapter from ckpts/d26d107eec32127ac86ef1997cf7169de1c56a59c539fc1258c6798b969e289c_LORA-BIAS-7B-v21.pth\n",
      "Trainable param: llama.layers.0.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.0.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.0.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.0.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.0.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.0.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.0.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.1.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.1.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.1.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.1.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.1.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.1.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.1.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.2.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.2.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.2.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.2.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.2.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.2.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.2.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.3.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.3.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.3.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.3.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.3.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.3.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.3.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.4.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.4.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.4.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.4.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.4.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.4.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.4.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.5.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.5.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.5.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.5.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.5.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.5.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.5.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.6.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.6.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.6.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.6.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.6.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.6.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.6.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.7.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.7.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.7.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.7.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.7.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.7.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.7.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.8.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.8.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.8.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.8.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.8.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.8.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.8.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.9.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.9.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.9.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.9.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.9.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.9.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.9.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.10.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.10.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.10.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.10.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.10.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.10.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.10.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.11.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.11.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.11.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.11.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.11.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.11.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.11.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.12.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.12.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.12.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.12.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.12.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.12.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.12.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.13.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.13.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.13.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.13.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.13.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.13.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.13.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.14.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.14.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.14.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.14.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.14.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.14.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.14.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.15.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.15.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.15.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.15.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.15.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.15.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.15.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.16.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.16.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.16.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.16.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.16.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.16.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.16.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.17.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.17.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.17.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.17.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.17.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.17.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.17.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.18.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.18.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.18.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.18.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.18.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.18.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.18.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.19.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.19.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.19.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.19.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.19.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.19.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.19.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.20.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.20.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.20.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.20.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.20.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.20.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.20.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.21.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.21.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.21.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.21.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.21.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.21.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.21.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.22.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.22.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.22.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.22.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.22.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.22.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.22.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.23.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.23.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.23.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.23.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.23.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.23.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.23.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.24.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.24.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.24.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.24.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.24.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.24.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.24.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.25.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.25.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.25.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.25.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.25.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.25.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.25.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.26.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.26.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.26.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.26.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.26.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.26.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.26.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.27.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.27.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.27.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.27.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.27.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.27.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.27.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.28.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.28.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.28.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.28.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.28.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.28.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.28.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.29.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.29.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.29.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.29.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.29.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.29.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.29.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.30.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.30.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.30.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.30.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.30.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.30.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.30.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.31.attention.wq.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.31.attention.wo.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.31.feed_forward.w1.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.31.feed_forward.w2.bias, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.31.feed_forward.w3.bias, torch.Size([11008]), torch.float32\n",
      "Trainable param: llama.layers.31.attention_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.layers.31.ffn_norm.weight, torch.Size([4096]), torch.float32\n",
      "Trainable param: llama.norm.weight, torch.Size([4096]), torch.float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LLaMA_adapter(\n",
       "  (clip): CLIP(\n",
       "    (visual): VisionTransformer(\n",
       "      (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (transformer): Transformer(\n",
       "        (resblocks): Sequential(\n",
       "          (0): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (12): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (13): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (14): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (15): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (16): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (17): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (18): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (19): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (20): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (21): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (22): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (23): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (token_embedding): Embedding(49408, 768)\n",
       "    (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (clip_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (clip_proj_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (visual_query): Embedding(10, 768)\n",
       "  (visual_blocks): ModuleList(\n",
       "    (0-7): 8 x Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (visual_proj): Linear(in_features=768, out_features=4096, bias=True)\n",
       "  (visual_proj_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  (adapter_query): Embedding(310, 4096)\n",
       "  (llama): Transformer(\n",
       "    (tok_embeddings): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "    (output): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "llama_dir = \"/home/Screenshot-LMM/llama-2\"\n",
    "\n",
    "# choose from BIAS-7B, LORA-BIAS-7B, LORA-BIAS-7B-v21\n",
    "model, preprocess = llama.load(\"LORA-BIAS-7B-v21\", llama_dir, llama_type=\"llama-2-7b\", device=device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = llama.format_prompt(\"Please describe this image.\")\n",
    "img = Image.fromarray(cv2.imread(\"./imgs/fastfood.jpeg\"))\n",
    "img = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "result = model.generate(img, [prompt])[0]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
